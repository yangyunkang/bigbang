{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>This notebook tries to detect \"special words\" in a corpus of mailing lists.</b>\n",
    "(for now it works with two mailing lists only)\n",
    "\n",
    "-it computes and exports in .csv files the word counts (words and their occurrences)\n",
    "-it computes and exports in .csv files the  list of common words that are introduced by different people in different lists\n",
    "it computes and print the 'influential words' (see definition in the box)\n",
    "\n",
    "Further extensions:\n",
    "-from two lists to n lists !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bigbang.archive import Archive\n",
    "from bigbang.archive import load as load_archive\n",
    "import bigbang.parse as parse\n",
    "import bigbang.graph as graph\n",
    "import bigbang.mailman as mailman\n",
    "import bigbang.process as process\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "import pytz\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "from itertools import repeat\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\davide\\bigbang\\bigbang\\archive.py:73: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  self.data.sort(columns='Date', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#insert TWO urls of mailing lists\n",
    "urls = [\"http://mm.icann.org/pipermail/cc-humanrights/\",\n",
    "       \"http://mm.icann.org/pipermail/wp4/\"]\n",
    "\n",
    "\n",
    "try:\n",
    "    arch_paths =[]\n",
    "    for url in urls:\n",
    "        arch_paths.append('../archives/'+url[:-1].replace('://','_/')+'.csv')\n",
    "    archives = [load_archive(arch_path) for arch_path in arch_paths]\n",
    "except:\n",
    "    arch_paths =[]\n",
    "    for url in urls:\n",
    "        arch_paths.append('../archives/'+url[:-1].replace('//','/')+'.csv')\n",
    "archives = [load_archive(arch_path) for arch_path in arch_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to stem or not to stem? \n",
    "#if stem is set to True, then words are converted into their stem(no plurals, no suffixes, etc.)\n",
    "#if stem is set to False, then words are processed for their literal spelling\n",
    "\n",
    "stem = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall compute the word counts on the lists. \n",
    "Data will be also exported to .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute word count on the first list\n",
    "wordcount={}\n",
    "for row in archives[0].data.iterrows():\n",
    "    if row[1][\"Body\"] is not None:\n",
    "        w = row[1][\"Body\"].replace(\"'\", \"\")\n",
    "        k = re.sub(r'[^\\w]', ' ', w)\n",
    "        t = nltk.tokenize.word_tokenize(k)\n",
    "        for g in t:\n",
    "            try:\n",
    "                if stem: word = st.stem(g)\n",
    "                else: word = g\n",
    "            except:\n",
    "                print g\n",
    "                pass\n",
    "            if word in stopwords.words('english'):\n",
    "                continue\n",
    "            if word not in wordcount:\n",
    "                wordcount[word] = [1]\n",
    "                wordcount[word].append(row[0])\n",
    "                wordcount[word].append(row[1][\"Date\"])\n",
    "                wordcount[word].append(row[1][\"From\"])\n",
    "                wordcount[word].append(row[1][\"In-Reply-To\"])\n",
    "            else:\n",
    "                wordcount[word][0] += 1\n",
    "wd = wordcount #In case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute word count on the second list\n",
    "wordcount1={}\n",
    "for row in archives[1].data.iterrows():\n",
    "    if row[1][\"Body\"] is not None:\n",
    "        w = row[1][\"Body\"].replace(\"'\", \"\")\n",
    "        k = re.sub(r'[^\\w]', ' ', w)\n",
    "        t = nltk.tokenize.word_tokenize(k)\n",
    "        for g in t:\n",
    "            try:\n",
    "                if stem: word = st.stem(g)\n",
    "                else: word = g\n",
    "            except:\n",
    "                print g\n",
    "                pass\n",
    "            if word in stopwords.words('english'):\n",
    "                continue\n",
    "            if word not in wordcount1:\n",
    "                wordcount1[word] = [1]\n",
    "                wordcount1[word].append(row[0])\n",
    "                wordcount1[word].append(row[1][\"Date\"])\n",
    "                wordcount1[word].append(row[1][\"From\"])\n",
    "                wordcount1[word].append(row[1][\"In-Reply-To\"])\n",
    "            else:\n",
    "                wordcount1[word][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exported!\n",
      "Check c:/users/davide/bigbang/wordcount_info_cc-humanrights.csv and c:/users/davide/bigbang/wordcount_info_wp4.csv\n"
     ]
    }
   ],
   "source": [
    "#Create and export a wordcount information dataframe per mailing list\n",
    "\n",
    "#set the variable 'path' as a valid directory path where to store the files\n",
    "path = 'c:/users/davide/bigbang/'\n",
    "\n",
    "\n",
    "asd = pd.DataFrame(wordcount)\n",
    "new_dataframe = asd.transpose()\n",
    "new_dataframe.columns = [\"Wordcount\", \"Message-ID\", \"Date\",\"From\",\"In-Reply-To\"]\n",
    "new_dataframe.to_csv(path+'wordcount_info_'+urls[0].split('/')[-2]+'.csv')\n",
    "\n",
    "asd1 = pd.DataFrame(wordcount1)\n",
    "new_dataframe1 = asd1.transpose()\n",
    "new_dataframe1.columns = [\"Wordcount\", \"Message-ID\", \"Date\",\"From\",\"In-Reply-To\"]\n",
    "new_dataframe1.to_csv(path+'wordcount_info_'+urls[1].split('/')[-2]+'.csv')\n",
    "\n",
    "print 'File exported!'\n",
    "print 'Check '+path+'wordcount_info_'+urls[0].split('/')[-2]+'.csv and '+path+'wordcount_info_'+urls[1].split('/')[-2]+'.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some useful descriptive data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in mailinglist https://mm.icann.org/pipermail/cc-humanrights/\n",
      "11665\n"
     ]
    }
   ],
   "source": [
    "print 'Number of unique words in mailinglist '+urls[0]\n",
    "print len(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in mailinglist http://mm.icann.org/pipermail/wp4/\n",
      "11665\n"
     ]
    }
   ],
   "source": [
    "print 'Number of unique words in mailinglist '+urls[1]\n",
    "print len(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of same unique words in two mailing lists\n",
      "3221\n"
     ]
    }
   ],
   "source": [
    "samewordcount=0\n",
    "for word in wordcount:\n",
    "    if word in wordcount1:\n",
    "        samewordcount += 1\n",
    "print 'Number of same unique words in two mailing lists'\n",
    "print samewordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of same words that are introduced by same people\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "samecount = 0\n",
    "for word in wordcount:\n",
    "    if word in wordcount1:\n",
    "        if wordcount[word][3] == wordcount1[word][3]:\n",
    "            samecount += 1\n",
    "print 'Total number of same words that are introduced by same people'\n",
    "print samecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among 100-500 appearance words, the number of common words between two mailing-list\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "samewordcount = 0\n",
    "for word in wordcount:\n",
    "    if wordcount[word][0] >= 100 and wordcount[word][0] <= 500:\n",
    "        if word in wordcount1:\n",
    "            if wordcount1[word][0] >= 100 and wordcount1[word][0] <= 500:\n",
    "                samewordcount += 1\n",
    "print 'Among 100-500 appearance words, the number of common words between two mailing-list'\n",
    "print samewordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among 100-500 appearance words, the number of common words between two mailing-list that are first introduced by same people\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "same_person_count = 0\n",
    "for word in wordcount:\n",
    "    if wordcount[word][0] >= 100 and wordcount[word][0] <= 500:\n",
    "        if word in wordcount1:\n",
    "            if wordcount1[word][0] >= 100 and wordcount1[word][0] <= 500:\n",
    "                if wordcount[word][3] == wordcount1[word][3]:\n",
    "                    #print word\n",
    "                    same_person_count += 1\n",
    "print 'Among 100-500 appearance words, the number of common words between two mailing-list that are first introduced by same people'\n",
    "print samecount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the list of common words that are introduced by different people in different lists.\n",
    "The results are exported in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common words introduced by different people in different lists\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "#compute common word list(introduced by different people in different lists)\n",
    "#and print the number\n",
    "commonwords = {}\n",
    "for word in wordcount:\n",
    "    if wordcount[word][0] >= 100 and wordcount[word][0] <= 500:\n",
    "        if word in wordcount1:\n",
    "            if wordcount1[word][0] >= 100 and wordcount1[word][0] <= 500:\n",
    "                if wordcount[word][3] != wordcount1[word][3]:\n",
    "                    commonwords[word] = [wordcount[word][0],wordcount[word][3],wordcount[word][2],\\\n",
    "                                         wordcount1[word][0],wordcount1[word][3],wordcount1[word][2]]\n",
    "print 'Number of common words introduced by different people in different lists'\n",
    "print len(commonwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exported!\n",
      "Check c:/users/davide/bigbang/commonwords_differentauthor.csv\n"
     ]
    }
   ],
   "source": [
    "#build dataframe of information of those words introduced by different people\n",
    "#and export to file\n",
    "df1 = pd.DataFrame(commonwords)\n",
    "commonword_differentauthor_dataframe = df1.transpose()\n",
    "commonword_differentauthor_dataframe.columns = [\"Wordcount1\", \"From1\", \"Date1\",\"Wordcount2\", \"From2\", \"Date2\"]\n",
    "commonword_differentauthor_dataframe.to_csv(path+'commonwords_differentauthor.csv')\n",
    "print 'File exported!'\n",
    "print 'Check '+path+'commonwords_differentauthor.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify \"influential words\" (see definition below) and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute 'influential words', the list of words that have potential of idea flows.\n",
    "\n",
    "#Definition: A is introduced by p in list1 first, then q saw it and then \n",
    "#introduced the word A to list 2, vice versa. We defined q saw as q said sth in list1 before p poped out the word. \n",
    "#Total list of such word A.\n",
    "\n",
    "\n",
    "#Build a dictionary with senders and date of first participation for each mailing list\n",
    "first_participation = {}\n",
    "for row in archives[0].data.iterrows():\n",
    "    if row[1][\"From\"] not in first_participation:\n",
    "        first_participation[row[1][\"From\"]] = row[1][\"Date\"]\n",
    "first_participation1 = {}\n",
    "for row in archives[1].data.iterrows():\n",
    "    if row[1][\"From\"] not in first_participation1:\n",
    "        first_participation1[row[1][\"From\"]] = row[1][\"Date\"]\n",
    "\n",
    "time_influence = 0\n",
    "influence_list = {}\n",
    "for word in commonwords:\n",
    "    if commonwords[word][2] > commonwords[word][5]: #Author2 comes first\n",
    "        if commonwords[word][1] in first_participation1: #Check if author1 in list2\n",
    "            if first_participation1[commonwords[word][1]] < commonwords[word][5]: #Check if author1\\\n",
    "                #in list2 and exists before the word first introduced in list2\n",
    "                influence_list[word] = commonwords[word]\n",
    "                time_influence += 1\n",
    "    else: #Author1 comes first\n",
    "        if commonwords[word][4] in first_participation:\n",
    "            if first_participation[commonwords[word][4]] < commonwords[word][2]:\n",
    "                influence_list[word] = commonwords[word]\n",
    "                time_influence += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No influential words detected\n"
     ]
    }
   ],
   "source": [
    "#print the list of influential words (exclude numbers)\n",
    "if len(influence_list.keys()) == 0: print 'No influential words detected'\n",
    "for word, info in influence_list.iteritems():\n",
    "    if not word.isdigit():\n",
    "        print '\"'+word+'\"'\n",
    "        print info\n",
    "        print ' '"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
